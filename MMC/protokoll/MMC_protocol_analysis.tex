\section{Data Analysis}

	% TODO@Christian: What is the bin-width and how did you determine it? Decide for one(!) time form: present OR simple past!
    According to the data evaluation, we got $N = 204$ events. That means, we got 204 times a distance, a time and a velocity which are distributed. 
    \subsection{Data evaluation of velocity}
        The mean of the velocity $\overline v$, its standard deviation $\sigma_{\overline v}$ and the standard error of the mean velocity $\Delta \overline v$ can be calculated by a data analysis tool. We used an Origin like software called qtiplot, which calculates column statistics, which are not influenced by binning, because that are the measured raw data. These include the searched values.
        In general one can calculate these values as following:
        \begin{eqnarray*}
            \overline v = N^{-1} \sum_{i = 1}^{N}v_i\ ,\ \sigma_{v} = \sqrt{\sum_{i = 1}^{N} \frac{(v_i - \overline v)^2}{N-1}}\ , \ \Delta \overline v = \frac{\sigma_{v}}{\sqrt{N}}
        \end{eqnarray*}
        So we get:
        $$ \overline v = \unit[(0.87 \pm 0.05)]{\mu m/s}\ , \ \ \sigma_{\overline v} = 0.72 \unit{\mu m/s}$$
        To get a better idea of the distribution of the values we created a histogram. Because we assume the velocity-values to be gaussian distributed we estimated a senseful bin-width $w_{bin}$ with the help of \textit{Scott's normal reference rule}\cite{wikiHisto}:
	    $$ w_{bin} = \frac{3.5 \cdot \sigma_v}{N} \cong 0.4\ \unit{\mu m/s}$$
	    The fit function choosed to approximate the probability-density of the velocity $v$ is given by:
	    \begin{align*}
	    		&pdf(v,A,\mu, w) = \frac{A}{w}\cdot \sqrt{\frac{2}{\pi}}\cdot e^{-2\cdot\left(\frac{v-\mu}{w}\right)^2}\\
	    		&A = 0.40\pm 0.01\\
	    		&\mu = (0.80 \pm 0.01)\ \unit{\mu m/s}\\
	    		&w = (0.75 \pm 0.03)\ \unit{\mu m/s}
	    \end{align*}
	    For fitting the cumulative distribution of the gaussian we used a built-in function that approximates the exact form:
	    \begin{align*}
		    &cdf(v,A,\mu,w) =\frac{A}{2}\cdot\left(1+\mathrm{erf}\left(\frac{\sqrt{2}\cdot(v-\mu)}{w}\right)\right)\\
		    &A = 0.991 \pm 0.002\\
		    &\mu = (0.595 \pm 0.006)\ \unit{\mu m/s}\\
		    &w = (0.71 \pm 0.02)\ \unit{\mu m/s}
	    \end{align*} 
	    Figure (\ref{velodist}) shows the resulting histogram for the relative frequency and its cumulative distribution including all the fits
        
                \begin{longtable}{p{8cm}p{8cm}}
                    \minipanf
                        A\\
                        \includegraphics[scale=0.3]{pic/velodist_rel}
                    \minipend
                    &
                    \minipanf
                        B\\
                        \includegraphics[scale=0.3]{pic/velocumdist}
                    \minipend
                \end{longtable}
                \captionof{figure}{\textbf{A:} Histogram of the relative velocity distribution. Bin-size: $0.4\ \unit{\mu m/s}$ \textbf{B:} Histogram of the corresponding cumulative velocity distribution}
                \label{velodist}
                \vspace{2mm}
        To minimize the error one could easily take more measurements. According to the law of large numbers the measured mean velocity would converge to the expactation value. With an infinite number of measurements we would get the exact result. 
        Also we could use software which marks the traces, which would create a unit law of marking. As a human being, one cannot see every trace and one can not mark every trace the way. This is caused by a diameter of the traces which is not infinite. The means, one could easily stretch the distance walked by the kinesin by marking from on edge to the diagonal opposite edge. An automization would avoid these errors. 
    
    \subsection{Data evaluation of run length}
    	Now we are going to figure out what distance $D$ a motorprotein covers on a single microtubule before releasing itself. For that we also use the data acquired by the streamed films. One frame of the stream corresponds to $150\ \unit{ms}$. With that and the determined velocity $\overline v = \unit[(0.87 \pm 0.05)]{\mu m/s}$ we can calculate the minimal distance we can measure with our streaming-system (and also the minimal bin-size): $d_{min} = \overline{v} \cdot 1\ \unit{FRAME} = 0.12\ \unit{\mu m} \cong 0.2\ \unit{\mu m}$. Therefore all of the measured distances which are below that value cannot be reasonable and they will be ignored in our statistics - anyway they will appear as first bin in the histogram for the sake of completeness. We rounded the value of $d_{min}$ up because we want to choose a smooth bin-width. Finally we decided the bin width to be $w_{bin} = 0.4\ \unit{\mu m} \cong 2 \cdot {d_{min}}$. We estimated this using the \textbf{Square-root choice}\cite{wikiHisto} which says that the number of bins is $k = \lfloor\sqrt{N}\rfloor$ where $N = 204$ is the number of samples.\\
    	Considering these facts we got the following results. At first the mean distance and its statistical error:
    	\begin{equation*}
    		\overline{D} = (1.1 \pm 0.1)\ \unit{\mu m}
    	\end{equation*}
    	The figures (\ref{exp:histRunLength}) and  (\ref{exp:cumuRunLength}) visualise the distribution of the measured distances:
    	 \minipanf
    	 	\centering
    	 	\captionsetup{justification=centering,margin=2cm}
    	 		\includegraphics[width = 0.8\textwidth, keepaspectratio]{pic/histo_runlength_rel.png}
    	 	\caption{Probibility distribution of the measured run length.\\ The very first bin is ignored from the exponential fitting.\\ Bin-size: $0.4\ \unit{\mu m}$}
    	 	\label{exp:histRunLength}
    	 \minipend\\
    	 \ \\   
    	 Because all invalid values fall into the first bin, it will be disregarded in the exponential fit of the probability function $P_D(d)$ and in the cumulative probability. We consider this fact by using an offset $d_0 = w_{bin} = 2 \cdot d_{min} = 0.4\ \unit{\mu m}$ in the exponential fit of the probability distribution. So the fit equation becomes:
    	 \begin{equation*}
    	 	P_D(d) = A \cdot e^{-\kappa\cdot(d-d_0)}
    	 \end{equation*}
    	 with the fit parameters:
    	 \begin{align*}
    	  	&A = 0.37 \pm 0.02\\
    	  	&\kappa = (1.2 \pm 0.1)\ \unit{\mu m^{-1}}\\
    	  	&d_0 = 0.4\ \unit{\mu m}= \mathrm{const.}
    	 \end{align*}
    	 
    	 \minipanf
    	     	     \centering
    	     	     \captionsetup{justification=centering}            
    	     	         \includegraphics[scale=0.3]{pic/cumulative_runlength.png}
    	     	     \caption{Cumulative probability disregarding the 'invalid' values with $d < d_0$}
    	    			 \label{exp:cumuRunLength}             
    	 \minipend\\
    	 \ \\
    	 For the cumulative distribution function $F_D(d) = P_D(D \leq d)$ we use the fit equation:\\
    	 \begin{equation*}
    	     	 	F_D(d) = 1- e^{-k\cdot(d-d_0)}
    	 \end{equation*}
    	 and extract the fit parameters:
    	 \begin{align*}
    	     	  	&k = (1.7 \pm 0.4)\ \unit{\mu m^{-1}}\\
    	     	  	&d_0 = 0.4\ \unit{\mu m}= \mathrm{const.}
    	 \end{align*}
    	With that equation we can choose an arbitary confidence level $F_D(d)\overset{!}{=}\pi(d) \in [0,1]$ which gives the probability of the run length $D$ to be less or equal than given $d$. If we invert this function we get the \textit{characteristic run length} $d = \pi^{-1}(\pi(d))$:
    	\begin{align*}
    			&d(\pi) = d_0 + \ln((1-\pi)^{-1/k})\\
    			&\Delta d(\pi) = \abs{\frac{\partial d}{\partial k}\Delta k} = \abs{\frac{\Delta k}{k}\cdot(d_0 - d)} 
    	\end{align*}
    	We calculate this length for some characteristic values of $\pi$ that correspond to the confidence levels of the Gaussian normal distribution for finding the value of the random variable $x$ in an intervall around the expencted value:  $x \in [-n\sigma + \mu, n\sigma + \mu],\ n=1,2,3$:\\
    	\ \\
    	\minipanf
    		\centering
    		\begin{tabular}{c|c|c|c}
    	 	       $n$ & $\pi$ & $d(\pi)\ [\unit{\mu m}]$  & $\Delta d(\pi)\ [\unit{\mu m}]$\\
    	 	 \hline		1	& 	0.6827	&	1.1 	& 	0.2\\
    	  				2	&	0.9545	&	2.2		&	0.4\\
    	  				3	&	0.9973	&	3.9		& 	0.8	
    		\end{tabular}
    	\minipend\\
    	\ \\
    	We prefered the cdf-method to the pdf-method because in that way we may check our result for given $\pi$ just by looking at the histogram without an additional integration. The deviation of the pdf-fit with respect to the histogram is also larger than in the cdf. We could minimise the error $\Delta d$ easily by taking more samples. We also should have avoided motorprotein-trajectories that are too 'short' to minimise the amount of values within the first bin. Also the magnification of the camera could be choosed larger to follow the trajectories better in the software.